# 卷积神经网络 #

<!-- toc -->

- [卷积神经网络](#卷积神经网络)
	- [1.神经网络](#1神经网络)
	- [2.卷积神经网络](#2卷积神经网络)

<!-- tocstop -->

&emsp;自今年七月份以来，一直在实验室负责卷积神经网络（Convolutional Neural Network，CNN），期间配置和使用过theano和cuda-convnet、cuda-convnet2。为了增进CNN的理解和使用，特写此博文，以其与人交流，互有增益。正文之前，先说几点自己对于CNN的感触。先明确一点就是，Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。


<div align=center>
<img src="http://ww2.sinaimg.cn/bmiddle/88070423gw1ep30aw8an7g204d04gkgd.gif" width="200" height="200" alt="亦菲表演机器猫"/>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
<img src="http://ww2.sinaimg.cn/bmiddle/88070423gw1ep30aw8an7g204d04gkgd.gif" width="200" height="200" alt="亦菲表演机器猫"/>
</div>

&emsp;
&emsp;
文／唐衣可俊（简书作者）
原文链接：http://www.jianshu.com/p/9d94660a96f1
著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。

- 第一点，在学习Deep learning和CNN之前，总以为它们是很了不得的知识，总以为它们能解决很多问题，学习了之后，才知道它们不过与其他机器学习算法如svm等相似，仍然可以把它当做一个分类器，仍然可以像使用一个黑盒子那样使用它。
- 第二点，Deep Learning强大的地方就是可以利用网络中间某一层的输出当做是数据的另一种表达，从而可以将其认为是经过网络学习到的特征。基于该特征，可以进行进一步的相似度比较等。

1. 包含序号样例1
2. 包含序号样例2


I get 10 times more traffic from [Google] [1] than from
[Yahoo] [2] or [MSN] [3].

  [1]: http://google.com/        "Google"
  [2]: http://search.yahoo.com/  "Yahoo Search"
  [3]: http://search.msn.com/    "MSN Search"

参考文献的引用：
如[参考文献1](#arm)


接下来话不多说，直接奔入主题开始CNN之旅

## 1.神经网络 ##
&emsp; 首先介绍神经网络，这一步的详细可以参考资源1。简要介绍下。神经网络的每个单元如下：

<center>
![NN](https://raw.githubusercontent.com/stdcoutzyx/Paper_Read/master/blogs/imgs/1.png)
</center>

其对应的公式如下：
                  $$ h_{W,b}(x) \:=\: f(W^Tx) \:=\: f(\sum_{i=1}^3W_ix_i \:+\: b) $$

&emsp;其中，该单元也可以被称作是Logistic回归模型。当将多个单元组合起来并具有分层结构时，就形成了神经网络模型。下图展示了一个具有一个隐含层的神经网络。
<center>
![LOG](https://raw.githubusercontent.com/stdcoutzyx/Paper_Read/master/blogs/imgs/3.png)
</center>
比较类似的，可以拓展到有2,3,4,5，…个隐含层。
**神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。关于训练算法，本文暂不涉及。**

>超链接示例

[卷积神经网络](http://blog.csdn.net/stdcoutzyx/article/details/41596663/ "卷积神经网络")

>上面是粗体示例，下面是斜体示例

>*代码示例*
```c++
int a = 9;
int b = 0;
```

```java
BufferedImage bfImage;//当然此处的bfImage不能为空了
ImageProcessor ip = new ColorProcessor(bfImage);//此处的使用ColorProcessor进行构造
```


## 2.卷积神经网络 ##
&emsp;在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。在上一节中提到的神经网络中，如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为1000000×1000000=10^12，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。就跟辟邪剑谱似的，普通人练得很挫，一旦自宫后内力变强剑法变快，就变的很牛了。
![enter image description here](https://raw.githubusercontent.com/stdcoutzyx/Paper_Read/master/blogs/imgs/10.gif)


---
>参考文献

<span id="arm">1. Davi L, Dmitrienko A, Sadeghi A R, et al. [Return-oriented programming without returns on ARM](http://www.trust.informatik.tu-darmstadt.de/fileadmin/user_upload/Group_TRUST/PubsPDF/ROP-without-Returns-on-ARM.pdf)[J]. System Security Lab-Ruhr University Bochum, Tech. Rep, 2010.</span>
